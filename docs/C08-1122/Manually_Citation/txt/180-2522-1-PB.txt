                                         Proceedings of the Third International ICWSM Conference (2009)




                              REVRANK: A Fully Unsupervised Algorithm for
                                    Selecting the Most Helpful Book Reviews

                                 Oren Tsur                                                   Ari Rappoport
                         Institute of Computer Science                                 Institute of Computer Science
                            The Hebrew University                                          The Hebrew University
                                Jerusalem, Israel                                             Jerusalem, Israel
                          oren@cs.huji.ac.il                                         www.cs.huji.ac.il/arir




                             Abstract                                            In this paper we describe a novel method for content anal-
                                                                             ysis, which is especially suitable for product reviews. Our
   We present an algorithm for automatically ranking user-
                                                                             system automatically ranks reviews according to their esti-
   generated book reviews according to review helpfulness.
                                                                             mated helpfulness. First, our REVRANK algorithm identi-
   Given a collection of reviews, our REVRANK algorithm iden-
                                                                             fies a core of dominant terms that defines a virtual optimal
   tifies a lexicon of dominant terms that constitutes the core of
                                                                             review. This is done in two stages: score terms by their fre-
   a virtual optimal review. This lexicon defines a feature vector
   representation. Reviews are then converted to this represen-              quency, and then identify the terms that are less frequent but
   tation and ranked according to their distance from a `virtual             contribute more information that is relevant to the specific
   core' review vector. The algorithm is fully unsupervised and              product. These terms are added to the core of the virtual op-
   thus avoids costly and error-prone manual training annota-                timal review. REVRANK then uses those terms to define a
   tions. Our experiments show that REVRANK clearly outper-                  feature vector representation of the optimal review. Reviews
   forms a baseline imitating the Amazon user vote review rank-              are then converted to this representation and ranked accord-
   ing system.
                                                                             ing to their distance from a `virtual core' review vector. Our
                                                                             method is fully unsupervised, avoiding the labor-intensive
                         Introduction                                        and error-prone manual training annotations typically used
                                                                             in content ranking tasks. We experimented with Amazon
The World Wide Web contains a wealth of opinions on just                     reviews on books from several genres, showing that our sys-
about anything. Online opinions come in various forms and                    tem clearly outperforms a baseline imitating the user vote
sizes, from short and informal talkbacks, through opinion-                   model used by Amazon.
ated blog postings to long and argumentative editorials. An                      The following section discusses related work. Next we
important source of information are postings in internet fo-                 present the details of the algorithm. The evaluation setup
rums dedicated to product reviews. In this era of user gener-                and results are given in the fourth section. In the Discussion
ated content, writing product reviews is a widespread activ-                 section we discuss and analyze different aspects of the re-
ity. People's buying decisions are significantly influenced by               sults. We conclude and present directions for future research
such product reviews. However, in many cases the number                      in the last section.
of reviews is rather large (there are many thousands of re-
views on the popular products), which causes many reviews                                           Related Work
to be left unnoticed. As a result, there is an increasing inter-
                                                                             Broadly taken, reader reviews can be thought of as essays
est in review analysis and review filtering, with the goal of
                                                                             having the reviewed product as their topic.         Higgins et
automatically finding the most helpful reviews.
                                                                             al. (2006) identify off-topic student essays based on lexical
   In order to help users find the best reviews, some websites
                                                                             similarity between essays in a collection of essays suppos-
(e.g., amazon.com) employ a voting system in which users
                                                                             edly on the same topic. Larkey (1998) used clustering and
can vote for review helpfulness ("was this review helpful
                                                                             regression models based on surface features such as average
to you? yes/no"). However, user voting mechanisms suf-
                                                                             length of essay, average length of word and number of dif-
fer from various types of bias (Liu et al. 2007), including
                                                                             ferent words, ignoring the content all together. Attali and
the imbalance vote bias (users tend to value others' opin-
                                                                             Burstein (2006) report on the evolution of e-rater, a com-
ions positively rather than negatively), the winner circle bias
                                                                             mercial grading system based on several models that analyze
(reviews with many votes get more attention therefore accu-
                                                                             discourse segments, stylistic features, grammar usage, lexi-
mulate votes disproportionately), and the early bird bias (the
                                                                             cal complexity and lexical similarity of essays on the same
first reviews to be published tend to get more votes).
                                                                             topic. They also provide a review of the field since its early
Copyright  2009, Association for the Advancement of Artificial
             c                                                               stages. All of these systems require labor-intensive annota-
Intelligence (www.aaai.org). All rights reserved.                            tion or a set of essays already graded by human graders.




                                                                       154

   An optimal helpful review could also be thought of as          POS taggers in a preprocessing stage, REVRANK does not
the best summary of many other reviews, each contribut-           need them. Avoiding such preprocessing systems increases
ing some insight to the optimal review. This is a type of         results quality, since these systems are usually trained on
multi-document summarization. In this sense, review rank-         well-written corpora, and thus tend to perform poorly on
ing is similar to the evaluation of multi-document sum-           freely-formed user generated content such as book reviews.
marization systems.     ROUGE (Lin and Hovy 2003) and                A second difference is that while the works above
Pyramid (Nenkova and Passonneau 2004) are the main                address electronic products, we focus on book reviews.
methods for evaluation of summarization systems. How-             Whereas electronic products have a relatively small number
ever, both suffer from the annotation bottleneck, as human-       of features discussed in reviews (typically found in semi-
produced summaries (and/or annotations) are required. Our         structured specification sheets), book reviewers tend to ex-
REVRANK needs no annotation at all since we use the vast          press themselves more poetically and to discuss many as-
number of online reviews as the human produced sum-               pects of authoring, such as author style, genre, plot, moral
maries. REVRANK is not a summarization system nor a               aspects of the story, existential feelings of the characters, and
summaries evaluation system per se although it lies between       ideas communicated by the author. Not only are these much
the two. There has been some work on review summariza-            harder to extract, they are hard to define. We find a flexible
tion: (Hu and Liu 2004) extract product features and output       set of key concepts, not only those specifically mentioned in
a sentiment-based summary-like list of product features and       the product specs and in pro/con lists.
sentences that describe them. Popescu and Etzioni (2005)             A third difference from some of the works above is in the
improve upon (Hu and Liu 2004).                                   evaluation method. While (Kim et al. 2006) measure suc-
   From a different angle, product reviews are opinions (sen-     cess by correlation with the Amazon user votes-based rank-
timents) on the product stated from various perspectives.         ing, we follow (Liu et al. 2007) in viewing this method as
The different perspectives expressed in documents were dis-       biased. We employed several human evaluators and show
tinguished based on their statistical distribution divergence     that our system outperforms the user-vote baseline.
in (Lin and Hauptmann 2006).                                         Finally, keyphrase extraction is another task relevant to
   Wiebe et al. (2004) learn to identify opinionated docu-        our work. (Mihalcea and Tarau 2004) propose the TextRank
ments by assigning a subjectivity score (learned from an an-      a graph-based ranking algorithm that ranks keyphrases in
notated corpus) to each document. Pang et al. (2002) com-         an unsupervised way according to concurrences links be-
pare different machine learning algorithms for sentiment          tween words. (Wan and Xiao 2008) propose CollabRank,
classification of movie reviews. Turney (2002) and Dave           a similar graph-based ranking model, using the collabora-
et al. (2003) classify reviews according to the polarity of the   tive knowledge given in multiple documents. Like Wan and
sentiment expressed. We are interested in helpfulness of the      Xiao (2008), REVRANK exploits a collection of documents
review as a whole and not only in what and how strong the         (all reviews for a given book) in order create the lexicon of
sentiment expressed in the review is.                             the dominant concepts. REVRANK can tolerate some noise,
   Some studies learn review quality. Liu et al. (2007) pre-      hence the lexicon is created in a much simpler way than both
pared and used a large manually annotated training set for        TextRank and CollabRank.
identifying low quality reviews of electronic products in a
supervised manner, in order to improve the summarization                        The REVRANK Algorithm
of sentiment regarding product features. Kim et al. (2006)
                                                                  Overview
predict the helpfulness of a review by structural features
such as length, lexical features, and meta-data such as rat-      REVRANK is based on a collaborative principle. Given mul-
ing summary (star rating at amazon.com). They use prod-           tiple reviews of a product, REVRANK identifies the most
uct features extracted from pro/con listings on epinions.com,     important concepts. The challenge lies in finding those con-
and sentiment words from publicly available lists. Review         cepts that are important but infrequent. The main idea em-
subjectivity (where "the reviewer gives a very personal de-       ployed by REVRANK is to use the given collection of re-
scription of the product") was used to predict helpfulness in     views along with an external balanced corpus in order to de-
(Ghose and Ipeirotis 2007).                                       fine a reference virtual core (VC) review. The VC review
   We differ from the works above in three main aspects.          is not the best possible review on this product, but is, in
First, our method is fully unsupervised, requiring no manu-       some sense, the best review that can be extracted or gener-
ally annotated training set. Liu et al. (2007) employed four      ated from the given collection (hence our usage of the term
annotators, each annotator following very detailed instruc-       `virtual': the collection might not contain a single review
tions in the course of annotating thousands of reviews for        that corresponds to the core review and the virtual core re-
training and evaluation. This is a serious project reporting      view may change with the addition of a single new review to
good results. However, having thousands of reviews an-            the collection). We do not generate the VC review explicitly;
notated and ranked by one evaluator might be problematic,         all reviews, including the VC one, are represented as feature
since after a few dozen reviews it is hard for the evaluator      vectors. The feature set is the lexicon of dominant terms
to assess the true helpfulness of a review due to heavy in-       contained in the reviews, so that vector coordinates corre-
terference of information from previous reviews. We also          spond to the overall set of dominant terms. Reviews are then
avoid the use of pre-made lists of sentiment words. While         ranked according to a similarity metric between their vectors
(Popescu and Etzioni 2005) use parsers, NER systems and           and the VC vector.




                                                               155

   Our approach is inspired by classic information retrieval,     ters and concepts related to the plot (e.g., `Langdon', `al-
where a document is represented as a bag of words, and each       bino' and `the holy grail' for the Da Vinci Code). There
word in each document is assigned a score (typically tf-idf       are also references to other relevant concepts, such as `Fou-
(Salton and McGill 1983)) that reflects the word's impor-         cault's Pendulum', `Angles and Demons', `Christianity',
tance in this document. The document is then represented by       `historical', `fictional' (again, for the Da Vinci Code). Ex-
a vector whose coordinates correspond to the words it con-        plicit handling of these semantic types will be addressed in
tains, each coordinate having the word's score as its value.      future papers.
Similarity of vectors denotes similarity of documents.               In order to identify the dominant terms, we use a balanced
   The key novelty in our approach is in showing how to           corpus B of general English (we used the British National
define, compute and use a virtual core review to address the      Corpus (BNC)). This resource is not specific to our problem
review ranking problem.                                           and does not require any manual effort. The key concepts
   Our features (dominant terms) constitute a compact lex-        are identified in the following manner. First we compute the
icon containing the key concepts relevant to the reviews of       frequency of all terms in the reviews collection. Each term
a specific product. The lexicon typically contains concepts       is scored by its frequency, hence frequent terms are consid-
of various semantic types: direct references to the book and      ered more dominant than others (stopwords are obviously
the plot, references to similar books or to other books by        ignored). Then, the terms are re-ranked by their frequency
the same author, and other important contextual aspects. We       in the reference corpus B. This second stage allows us to
identify this lexicon in an unsupervised and efficient man-       identify the concepts that serve as key concepts with respect
ner, using a measure motivated by tf-idf with the use of an       to the specific book. For example, the term `book' or `Dan
external balanced reference corpus.                               Brown', the name of the author of the Da Vinci Code are
   There are three main differences from the classic tf-idf.      usually very frequent in the reviews corpus, however their
First, at the stage of key concepts extraction, we view the       contribution to the helpfulness of a review is limited as they
collection of reviews for a specific book as a single long        do not provide the potential reader with any new information
document. Second, instead of computing inverted frequen-          or any new insights beyond the most trivial. On the other
cies on the reviews corpus, we use an external balanced ref-      hand, concepts like 'Fuocault's Pendulum' and `the Council
erence corpus. Finally, instead of counting the documents in      of Nicea' are not as frequent but are potentially important,
the inverted corpus, we compute the term-frequency in the         therefore the scoring algorithm should allow them to gain a
external balanced corpus. The rationale behind this is that       dominance score.
we first want to identify the set of key concepts in reviews of      Given a corpus R      consisting of reviews for product p,
                                                                                         p
a specific book, instead of directly retrieving a single docu-    these two stages can be collapsed so the dominance of a lex-
ment/review.                                                      ical item t in R is given by:
                                                                                   p
   Using standard tf-idf by viewing each review as a docu-
ment, we would perhaps be able to discover important terms                                                    1
that appear in very few reviews, but we might not be able                         D    (t) = f                                  (1)
                                                                                    Rp         Rp(t) · c ·log B(t)
to discover central terms that are common to several reviews
(since the idf tends to punish terms that appear in more than        where f    (t) is the frequency of term t in R , B(t) is the
                                                                             Rp                                      p
one review). Reviews that make use of such terms are ex-          average number of times t appears per one million words in
pected to be more helpful than reviews that do not, because       the balanced corpus (BNC in our case), and the constant c is
in a sense the existence of such terms guarantees that the        a factor used to control the level of dominance . The lexicon
                                                                                                                    1

review also covers the main aspects of the subject at hand.       is constructed ignoring stopwords.
   The REVRANK ranking algorithm has three simple                    Equation 1 is designed to capture the key concepts that are
stages, described in detail below: (i) identify dominant          dominant in the collection R , balancing the bias induced by
                                                                                                 p
terms; (ii) map each review to a feature vector defined by        frequent words, capturing words of different semantic types
the dominant terms; and (iii) use some similarity metric to       and capturing words that are dominant but infrequent.
rank the reviews.
                                                                     Once each term has a dominance score, we choose the m
                                                                  most dominant lexical items to create a compact virtual core
The Virtual Core Review
                                                                  review. Based on a small development set (reviews for only
Lexical items (one word or more) are associated with the          two books not participating in the test), we used m = 200.
virtual core review according to their dominance. As with         Clearly, m could be determined dynamically to achieve op-
tf-idf, dominant terms are not necessarily the most frequent      timal results for different products, but we leave this for fu-
terms in the reviews collection. The dominant concepts usu-       ture research. In principle, lexical items may have different
ally cover a wide range of semantic types (to clarify, we do      dominance weights (e.g., given by their score or by their se-
not determine these types explicitly here).                       mantic type), allowing several specific ranking schemes. In
   Important types include words common to product re-
views in general, such as `excellent'; words common to re-           1It should be tuned according to the balanced corpus used. In
views of certain types of products (e.g., books and movies),      our experiments we used c = 3, so that terms with frequency lower
such as `boring' and `masterpiece'; words common to re-           than 0.1% in the BNC will have c ·        1
                                                                                                       log BNC(t)  > 1. This way,
views of books of certain genres, such as `thriller' and `non-    when using 1 as a dominance threshold, less terms are considered
fiction'; book specific terms such as names of main charac-       dominant as c increases.




                                                               156

this paper, we set all weights to be 1, which gives excellent         The evaluation of review ranking challenges us with some
results (see the `Evaluation Setup and Results' Section).          other issues that are worth mentioning. A good product
                                                                   review is one that helps the user in making a decision on
Review Representation and the Virtual Core                         whether to buy or use the product in question. In that sense,
Review                                                             the evaluation of reviews is even more problematic. A po-
                                                                   tential buyer is really interested in the product and we may
The compact lexicon of the m most dominant concepts is
                                                                   assume that the way she approaches the immense number of
now used for defining a feature vector representation of re-
                                                                   available reviews is mentally different from the mental ap-
views. Coordinates of the vector correspond to the dominant
concepts. Each review r  Rp is mapped to vr in this rep-           proach of an objective evaluator who has no real interest in
                                                                   the product. We refer to this problem as the motivation is-
resentation such that a coordinate k is 1 or 0 depending on
                                                                   sue. For example, it might be the case that a real buyer will
whether or not the review r contains the kth dominant term.
                                                                   find interest in a poor review if it adds to the information he
   We refer to the feature vector having 1 in all of its coordi-
                                                                   already has.
nates as the virtual core feature vector (VCFV). All reviews
that make use of all of the dominant terms are represented
                                                                      Finally, the evaluation procedure of this type of tasks is
by VCFV. In practice, it is rare for a review to use all of the
                                                                   rather expensive. We propose an evaluation procedure that
dominant terms, which is why we call it `virtual'.
                                                                   balances these constraints.
   Note that VCFV only represents the set of `core' concepts
in the given review collection, and may dynamically change            There are two main approaches to deal with review rank-
as more reviews are added to the collection.                       ing, none of which fully overcomes the motivation issue.
   Again, in both the VCFV and all of the v 's weights are
                                              r                    The first approach, as used in (Liu et al. 2007), is to de-
all set to 1, which could be extended in the future.               fine strict and detailed guidelines and have the human anno-
                                                                   tators annotate and evaluate the reviews according to those
Review Ranking                                                     guidelines. Liu et al. (2007) used 4 annotators, each anno-
                                                                   tating and evaluating thousands of reviews. Given a review,
A good book review presents the potential reader with
                                                                   their annotators were asked to indicate whether a decision
enough information without being verbose. Therefore each
                                                                   could be made based on this review solely, whether the re-
review r is given a score by the following equation:
                                                                   view is good but more info is needed, whether the review
                                1      d                           conveys some useful piece of information although shallow,
                     S(r) =             r
                                                            (2)
                              p(|r|) · |r|                         or whether the review is simply useless. Due to the usage of
                                                                   strict rules, this method is a good one. However, the evalua-
where d = v · V CF V is the dot product of the (weighted)
         r     r                                                   tion is still subjective and the motivation issue is still present.
representation vector of review r and VCFV, |r| is the num-        In addition, using a small number of annotators supposedly
ber of words in the review and p(|r|) is a punishment factor       ensures consistency, but could also result in a kind of `early
given by:                                                          bird' bias, since annotators become product experts when
                 p(|r|) = {1c       |r|<|r|                 (3)    reading so many reviews. For these reasons, this methodol-
                                    otherwise
                                                                   ogy is more suitable to products such as electronic products,
The punishment factor is needed in order to punish reviews
                                                                   which have a relatively small number of well-defined fea-
that are too short or too long. We set c = 20, to deliberately
                                                                   tures, than to books, movies and music, on which opinions
create a high threshold for reviews that are too short. This
                                                                   can be highly open ended.
arbitrary decision was based on our familiarity with Amazon
book reviews; the function p() could be adjusted to punish or         In the other approach, instead of having a small number
favor long reviews according to user preferences (see Equa-        of evaluators evaluate a very large number of reviews ac-
tion 5) or to corpus specific (or forum specific) characteris-     cording to strict guidelines, we could have a large number
tics. In our experiments we assumed that users tend to get         of evaluators, each evaluating a small number of reviews ac-
annoyed by verbose reviews; the punishment for an exces-           cording to loosely-defined guidelines. While more expen-
sive length is already given by the denominator |r| (Equa-         sive, this approach is closer to amazon's voting system, in
tion 2).                                                           which many potential buyers read a relatively small number
                                                                   of reviews and indicate whether they are useful or not. When
             Evaluation Setup and Results                          applied to book reviews, this method is more suitable to deal
                                                                   with the motivation issue, since the random evaluator who is
The Nature of Book Review Evaluation
                                                                   not actively interested in getting the product relates to book
The evaluation of tasks such as review ranking is known to         reviews much easier than to the technical details and lingo
be problematic. The main issue is the need for human as-           of the review of some electronic gadget.
sessors in order to evaluate tasks. Not only is the definition
of a good output vague, success in these tasks is highly sub-         Both methods have their pros and cons. We chose the
jective. One way to overcome these issue is by having more         second, both because it is closer to the real conditions at
than one evaluator, then using measures for inter-evaluator        which ranking reviews according to helpfulness is needed
agreement in order to estimate evaluation reliability (Liu et      and because it is more suitable to the type of products we
al. 2007).                                                         wanted to address (books).




                                                                157

Evaluation Procedure                                                                Book    NoR    Length   Votes   Dom

                                                                                    DVC     3481    175.3    15.5   11.1
Obviously, the forum a review is posted at influences the                           WiF     1025    195.4    14.1   14.4
expectations of the users (its potential readers). For this re-                      HP     5000    182.8     3.4   17.3

search (for our human subjects) we define a helpful review in                        EG     2433    146.5     2.6   15.9

a somewhat loose manner as follows : A good (helpful) book
                                       2                                            TSS      784    120       4.6   14.8

review presents the potential reader with sufficient informa-
tion regarding the book's topic/plot/writing style/context or       Table 1: Corpus Data. NoR: Number of Reviews. Length:
any other criteria that might help him/her in making a ratio-       average number of words in a review. Votes: the average
nal decision on whether or not s/he wants to read or buy the        number of votes (both positive and negative votes) given for
book.                                                               a review of the book by Amazon users. Dom: the average
   We compared user votes-based ranking to our REVRANK              number of dominant lexical features found by REVRANK in
ranking, and assessed performance by asking human evalua-           a review. DVC: The Da Vinci Code; WiF: The World is Flat;
tors to rate the reviews. User votes-based ranking takes two        HP: Harry Potter and the Order of the Phoenix; EG: Ender's
factors into account: the total number of votes a review re-        Game; TSS: A Thousand Splendid Suns.
ceives, and the ratio between the number of helpful votes and
the total number of votes. Both factors are needed since a re-                Book     IE Agreement  kappa, d = 2   kappa, d = 1
view that has a single `helpful' vote (100% of the votes are                  DVC          75%           0.75           0.57
positive) cannot be automatically considered as more help-                    WiF          79%           0.79           0.65

ful than a review with helpful/total ratio of 281/301. We                     HP5          83%           0.69           0.5

tried to approximate the user votes based-ranking employed                    EG           71%           0.71           0.46

by Amazon, ranking reviews that have more than 15 votes                       TSS          58%           0.64           0.57

such that their helpful/total ratio is over 0.5 higher than re-             Overall       73.3%          0.69           0.56

views with a slightly better ratio but under 15 total votes. In
a similar way, we ranked reviews with more than 30 votes            Table 2: Percentage of inter-evaluator agreement for d = 2
and a majority of positive votes above those with less than         and Fleiss' kappa statistics for d = 1, 2. The kappa statistic
30 votes. We do not claim that this is the exact algorithm          shows significant agreement for both ds. For 73.3% of the
used by Amazon, but we verified (on hundreds of reviews)            total of 120 evaluated reviews, there was complete agree-
that it is close enough for our purposes.                           ment between three evaluators.
   We tested our system on five books from five different
genres, using the Amazon API to obtain the reviews. The
                                                                    views , two reviews were randomly sampled from the top
                                                                           3
books are: The Da Vinci Code, historical fiction (by Dan
                                                                    2 (or 5)% of reviews as ordered by our REVRANK system,
Brown); the World is Flat, non-fiction (by Thomas Fried-
                                                                    and two reviews (control set) were randomly sampled from
man); Harry Potter and the Order of the Phoenix, fantasy
                                                                    the reviews that are not included in the top 2 (or 5)% reviews
(by JK Rawlings); Ender's Game, science fiction (by Or-
                                                                    of the user votes-based ranking (some of them could be very
son Scott Card), and A Thousand Splendid Suns, fiction (by
                                                                    helpful, but they had not accumulated enough votes).
Khaled Hosseini). All five books are worldwide bestsellers
                                                                       Each batch of six reviews was given to three human evalu-
and enjoy many Amazon reviews. Table 1 presents some
                                                                    ators to rank the reviews according to helpfulness, assigning
details about the reviews of these books. The table shows
                                                                    6 to the most helpful review and 1 to the least helpful review.
that the books differ in almost every aspect: genre, number
                                                                       Overall, we sampled 24 reviews for each book, 120 re-
of reviews, average review length, and average number of
                                                                    views in total. Each review was evaluated by three different
votes.
                                                                    evaluators, having 360 evaluations. Note that each evaluator
   The purpose of our evaluation is three-fold. First, we ver-
                                                                    was given only one batch of 6 reviews for a book, in order
ify that user votes helpfulness ranking is biased. Second,
                                                                    to prevent him/her from being cognitively loaded with infor-
we demonstrate that ranking based on REVRANK succeeds
                                                                    mation in a way that might interfere with his/her judgment.
in finding helpful reviews among Amazon reviews that re-
ceived no user votes at all. Finally, we show that REVRANK
                                                                    Inter-evaluator Agreement
ranking is preferable even when comparing to the top 2%
                                                                    Review evaluation is subjective. However, a strong inter-
reviews ranked according to user votes.
                                                                    evaluator agreement observed for a number of evaluators
   The evaluation process is as follows. For each of the five
                                                                    evaluating each set of reviews can indicate that one system
books, we created four batches of six reviews each. Each
                                                                    is superior to another (see (Liu et al. 2007) regarding review
batch was compiled as follows: two reviews were randomly
                                                                    ranking and the extensive literature regarding the evaluation
sampled from the top 2% or 5% of the user votes-based re-
                                                                    of summarization and question answering systems). Inter-
                                                                    evaluator agreement in this evaluation task is somewhat
   2 This definition also complies with Amazon's review writing
tips ("What makes a good review?"), indicating: "be detailed and       3To reduce noise, our preference was to use the top 5%. How-
specific. What would you have wanted to know before you pur-        ever, in three of the books in our experiments, only the top 2% re-
chased the product?". Amazon's guidelines are presented to review   views had more than 15 votes each with a helpfulness ratio higher
writers, and ours to our evaluators.                                than 0.5.




                                                                 158

             Book      System  Length   Votes  Dom

                        UV      308.6   103.8  16.7
             DVC
                        RVR     279.1   51.4   24.7

                        UV      335.5   94.4   14.1
              WiF
                        RVR     334.4   10.6   25.6

                        UV      345.4   51.2   27.7
               HP
                        RVR     211.8   1.7    30.7

                        UV      379.5   60.52  31.3
               EG
                        RVR     188.9   2.3    33.1

                        UV      400.8   46.5   34.6
              TSS
                        RVR     142.7   3.5    24.5

                        UV      353.8   71.3  124.4
            All books
                        RVR     202.8   13.8   27.7



Table 3: Averages over the top reviews of the user votes-
based ranking (UV) and our REVRANK system.              Votes:   Figure 1: Average evaluation score for the REVRANK sys-
number of votes (positive and negative). Dom: dominant           tem, Amazon user vote-based baseline, and a random sam-
lexical features.                                                ple. Note that the optimal score for a ranking system can
                                                                 only be 5.5, in case it wins the top two places (5 and 6).
                                                                 Similarly, the lowest possible score is 1.5.
tricky. Each evaluator is asked to assign each review with a
number between 1 and 6. Reviews differ in style and cover-
age. Evaluators differ in their expectations and tolerance for
                                                                 chosen by the user votes (UV) based algorithm and the re-
different review styles (one evaluator indicated she prefers
                                                                 views chosen by REVRANK, with an average length of 358.8
cynical reviews while other explained that a certain review
                                                                 for the UV and only 202.8 for REVRANK.             Looking at
is `too cynical'). Given the fact that the range of evaluation
                                                                 both the average length and the average number of domi-
scores was 1 to 6, and given the different personal prefer-
                                                                 nant terms found in the reviews shows that REVRANK re-
ences of the evaluators and the open style of review writing,
                                                                 views convey much more information in much shorter re-
it is beyond our expectation that evaluators will show strict
                                                                 views. Our results clearly show that users (human evalua-
uniformity. It is therefore reasonable to define a softer mea-
                                                                 tors) prefer these compact reviews. This observation will be
sure of agreement. We define agreement as follows. Denote
                                                                 discussed in greater detail in the Discussion section.
by e the three evaluators of review r, and by score the func-
     i                                                              Examining the evaluators' choice of a single most helpful
tion score(e, r) : {e1, e3, e3} × {r  R} - {1, ..., 6}.          review in each batch shows that an REVRANK choice was
Then agr(e1, e2, e3) := 1 iff                                    ranked the most helpful in 85% of the batches.
         i¬j         score(e , r) - score(ej, r) > d       (4)      Comparing our subjects' appraisal of user-based ranking
                             i
                                                                 to the control set shows that the former is only slightly better
Otherwise, agr(e1, e2, e3) = 0. Since scores vary from 1 to      than the latter. We attribute this to the fact that the random
6, we set d = 2, assuming that if all 3 evaluators scored r      sample may contain quality reviews that have not accumu-
within 2 scores distance we can say there is an agreement.       lated enough votes, therefore were not part of the UV sample
   Fleiss' kappa is a common statistic that measures agree-      space. These results, along with the difference in the average
ment between multiple evaluators. Table 2 shows the ratio        number of votes (Table 3), confirm that user-based ranking
of the agreement and the Fleiss' kappa statistic for d = 2       could be problematic.
and d = 1, demonstrating substantial agreement for d = 2
and quite a good agreement for d = 1.                                                    Discussion
                                                                 Although our system performs significantly better than the
Results
                                                                 user vote-based ranking system, there is one book (A Thou-
Figure 1 presents the performance of our system compared         sand Splendid Suns) on which both are evaluated similarly.
to the user votes ranking and to a random sample. Our sys-       We note that even a similarity in the evaluation score proves
tem significantly outperforms both of these methods. Note        the benefits of REVRANK since the system (a) is unsuper-
that the maximal grade is 5.5, not 6, because every batch        vised, and (b) finds reviews otherwise overlooked by the
contained two reviews selected by each competing system,         users. This result can be explained by differences in aver-
so the best a system can do is win the first two places with     age review length. Table 1 shows that while the average
grades of 6 and 5. Table 3 shows that the average number of      length of reviews for the first four books is 175 words per
votes for the top reviews ranked by REVRANK is only 13.8,        review, the average length for A Thousand Splendid Suns
while the top user-based reviews have an average of 71.3         is only 120 words. The REVRANK ranking algorithm bal-
votes. This shows that our system indeed recognizes worthy       ances the length of the review and the number of pieces of
reviews overlooked by users.                                     information it conveys, and as such, it favors shorter reviews
   As can be viewed in Table 3, another interesting obser-       relative to the average length of the reviews for the book
vation is the large difference in length between the reviews     in question (see Equation 2). Apparently, human evaluators




                                                              159

    Flattening Flatteners are flattening the world...
    .                                                                 probably no one best review, (b) review evaluation is after
    I listened to Tom Friedman's book on CD, and this is what         all subjective, and (c) reviews can complement each other,
    it sounded like: "I realized the world was flat when the          providing an added value to the potential reader. Users typi-
    flattening flatterers converged and flattened the world into      cally read more than the very top review for a product. The
    flatness. Flatteners one through ten flattened the world          algorithm could be easily configured to output the top n or-
    but when they converged into convergence with a TCP/IP            thogonal reviews that have the least overlap of key concepts.
    SOAP XML in-source outsource delivery flattening, the
                                                                      This will present the user with a set of complementary re-
    flatness became overriddingly flat and converged..."
                                                                      views. Doing so directs the effort toward multi-review sum-
    In fairness, the book is a good piece of scholarship and
                                                                      marization, a good subject for future work.
    shoe leather, and Friedman does help educate a good por-
    tion of the world on how all these forces intertwine. But
    hearing the book on CD makes you see how Friedman                                         Conclusion
    hammers this stuff over and over again like a two-by-four
    to the middle of your forehead.                                   Book reviews greatly vary in style and can discuss a book
                                                                      from very many perspectives and in many different contexts.
                                                                      We presented the REVRANK algorithm for information min-
   Table 4: Excerpt of review #664 for The World is Flat.
                                                                      ing from book reviews. REVRANK identifies a compact lex-
                                                                      icon (the virtual core review) that captures the most promi-
(and readers) tend to prefer reviews that are longer than a           nent features of a review along with rare but significant fea-
certain constant, 10 sentences or 150 words approximately .      4    tures. The algorithm is robust and performs well on different
This can be easily supported by REVRANK by adjusting the              genres. The algorithm is fully unsupervised, avoiding the
punishment function (Equation 3) to be:                               annotation bottleneck typically encountered in similar tasks.
                                                                      The simplicity of REVRANK enables easy understanding of
             p(|r|) = {c         |r|<|r|     or    |r|<l        (5)   the output and an effortless parameter configuration to match
                          1       otherwise
                                                                      the personal preferences of different users.
where l is a constant determining a lower bound on the
                                                                        For future work we plan to experiment with even smaller
length of a helpful review. Equation 5 could be also used
                                                                      review sets. In addition, we will study the effects of different
to fit the preferences of a specific user who prefers shorter
                                                                      weighting schemes in order to enable a personalized ranking
or longer reviews.
                                                                      according to various criteria. Finding the optimal set of or-
   We note that the hypothesis about the length preference
                                                                      thogonal reviews will also be studied. We will eventually
of the user is supported by our results, however we are not
                                                                      generate one comprehensive review from the most valuable
aware of any cognitive or computational work that specifies
                                                                      orthogonal reviews chosen by our ranking system. The ap-
a length of preference. Length preference is subjective and
                                                                      plication of the algorithm on the blogosphere as well as other
may vary between domains. We leave tuning of length pref-
                                                                      social media domains can also be investigated, for creating
erences to a future study.
                                                                      a vertical search engine that identifies the most helpful re-
   While the reviews selected by REVRANK obtained an av-
                                                                      views on the web and not only on a restricted corpus. Fi-
erage rank of 4.5, review 664 for The World is Flat, sampled
                                                                      nally, the expensive nature of the evaluation process raises
for the 6th batch, has an average evaluator rank of 2 (two
                                                                      an essential direction of future work: developing a gold stan-
of the three evaluators ranked it as the least helpful review
                                                                      dard for evaluation purposes.
in the batch). Looking at the review itself reveals that this
review is a parody, criticizing the book's extensive and repet-
itive use of buzz words (see Table 4). The reviewer used all
the right terms but deliberately misplaced them in order to           Acknowledgements We would like to thank Prof. Moshe
form sentences that are extremely incoherent, hence not pre-          Koppel for his insightful comments.
senting the reader with any explicit information about the
book. This review was ranked relatively high by REVRANK
                                                                                              References
due to the fact that the system sees the reviews as a bag of
words, and is thus insensitive to the order of the words. It is        Attali, Y., and Burstein, J. 2006. Automated essay scoring
also worth noting that this review got 15 votes from Ama-              with e-rater v.2. The Journal of Technology, Learning and
zon users, 13 of which were `helpful'. These votes make a              Assesment 4(3).
helpfulness ratio of 87%. The review did not obtain such               Dave, K.; Lawrence, S.; and Pennock, D. M. 2003. Mining
a high helpful score by the user-based baseline because it             the peanut gallery: opinion extraction and semantic classi-
did not accumulate enough votes (over 30), but apparently              fication of product reviews. In WWW '03: Proceedings
many readers liked the parody and thought that the message             of the 12th international conference on World Wide Web,
it conveys is helpful.                                                 519­528. New York, NY, USA: ACM.
   Another aspect worth mentioning is that REVRANK is
                                                                       Ghose, A., and Ipeirotis, P. G.     2007. Designing novel
best viewed as a tool for finding the top n reviews and not
                                                                       review ranking systems: predicting the usefulness and im-
the one best review. This is due to the facts that (a) there is
                                                                       pact of reviews. In ICEC '07: Proceedings of the ninth in-
    4Amazon review writing tips suggest "Not too short and not too     ternational conference on Electronic commerce, 303­310.
long. Aim for between 75 and 300 words".                               New York, NY, USA: ACM.




                                                                   160

Higgins, D.; Burstein, J.; and Attali, Y. 2006. Identify-         Turney, P. D. 2002. Thumbs up or thumbs down? se-
ing off-topic student essays without topic-specific training      mantic orientation applied to unsupervised classification of
data. Nat. Lang. Eng. 12(2):145­159.                              reviews. In ACL '02: Proceedings the 40th annual meeting
                                                                  of the ACL, volume 40, 417.
Hu, M., and Liu, B. 2004. Mining and summarizing cus-
tomer reviews. In KDD '04: Proceedings of the tenth ACM           Wan, X., and Xiao, J. 2008. Collabrank: Towards a col-
SIGKDD international conference on Knowledge discov-              laborative approach to single-document keyphrase extrac-
ery and data mining, 168­177.        New York, NY, USA:           tion. In Proceedings of the 22nd International Conference
ACM.                                                              on Computational Linguistics (COLING).

Kim, S.-M.; Pantel, P.; Chklovski, T.; and Pennacchiotti,         Wiebe, J.; Wilson, T.; Bruce, R.; Bell, M.; and Martin,
M. 2006. Automatically assessing review helpfulness. In           M. 2004. Learning subjective language. Computational
Proceedings of the Conference on Empirical Methods in             Linguistics 30(3):277­ 308.
Natural Language Processing (EMNLP), 423­430.

Larkey, L. S. 1998. Automatic essay grading using text cat-
egorization techniques. In Proceedings of SIGIR-98, 21st
ACM International Conference on Research and Develop-
ment in Information Retrieval, 90­95.      Melbourne, AU:
ACM Press, New York, US.

Lin, W.-H., and Hauptmann, A. 2006. Are these documents
written from different perspectives?: a test of different per-
spectives based on statistical distribution divergence. In
ACL '06: Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual meeting
of the ACL, 1057­1064. Morristown, NJ, USA: Associa-
tion for Computational Linguistics.

Lin, C.-Y., and Hovy, E.      2003.  Automatic evaluation
of summaries using n-gram co-occurrence statistics.        In
NAACL '03: Proceedings of the 2003 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 71­78.
Morristown, NJ, USA: Association for Computational Lin-
guistics.

Liu, J.; Cao, Y.; Lin, C.-Y.; Huang, Y.; and Zhou, M. 2007.
Low-quality product review detection in opinion summa-
rization.   In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EMNLP-
CoNLL), 334­342.

Mihalcea, R., and Tarau, P. 2004. Textrank: Bringing order
into texts. In Conference on Empirical Methods in Natural
Language Processing.

Nenkova, A., and Passonneau, R. J. 2004. Evaluating con-
tent selection in summarization: The pyramid method. In
HLT-NAACL, 145­152.

Pang, B.; Lee, L.; and Vaithyanathan, S. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of the 2002 Conference on Empir-
ical Methods in NaturalLanguage Processing (EMNLP),
79­86.

Popescu, A.-M., and Etzioni, O. 2005. Extracting product
features and opinions from reviews. In HLT '05: Proceed-
ings of the conference on Human Language Technology
and Empirical Methods in Natural Language Processing,
339­346. Morristown, NJ, USA: Association for Compu-
tational Linguistics.

Salton, G., and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill New York, NY.




                                                              161

